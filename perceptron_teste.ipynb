{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = './datasets/test.csv'\n",
    "train_file_path = './datasets/train.csv'\n",
    "test_df = pd.read_csv(test_file_path)\n",
    "train_df = pd.read_csv(train_file_path)\n",
    "\n",
    "# Remove the first column\n",
    "np_train_df = train_df.to_numpy()[:, 1:]\n",
    "np_test_df = test_df.to_numpy()[:, 1:]\n",
    "\n",
    "# CONSTRAINTS\n",
    "DIGIT = 0\n",
    "FIRST_PIXEL = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_pixel(size, i ,j):\n",
    "  return i*size + j\n",
    "\n",
    "def symmetry(image):\n",
    "  size = 28\n",
    "  sv = 0\n",
    "  sh = 0\n",
    "  \n",
    "  for i in range(size):\n",
    "    for j in range(int(size/2)):\n",
    "      p1 = image[pos_pixel(size, i, j)]\n",
    "      p2 = image[pos_pixel(size, i, (size-1)-j)]\n",
    "      sv += abs(p1 - p2)\n",
    "            \n",
    "      p1 = image[pos_pixel(size, j, i)]\n",
    "      p2 = image[pos_pixel(size, (size-1)-j, i)]\n",
    "      sh += abs(p1 - p2)\n",
    "\n",
    "  sv = sv/255\n",
    "  sh = sh/255\n",
    "\n",
    "  s = sv + sh\n",
    "  \n",
    "  return s\n",
    "  \n",
    "def intensity(image):\n",
    "  return (np.sum(image)/255.0)\n",
    "\n",
    "\n",
    "def get_numbers_1_and_5(df):\n",
    "  filtered_data = []\n",
    "\n",
    "  for item in df:\n",
    "    if(item[0] == 1 or item[0] == 5):\n",
    "      filtered_data.append(item.tolist())\n",
    "\n",
    "  return filtered_data\n",
    "\n",
    "def add_1_column(X):\n",
    "  ones_list = map(lambda i: [1, i[0], i[1]], X)\n",
    "  return np.array(list(ones_list))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = []\n",
    "images = get_numbers_1_and_5(np_train_df)\n",
    "\n",
    "for image in images:\n",
    "  symmetry_ = symmetry(image[FIRST_PIXEL:])\n",
    "  intensity_ = intensity(image[FIRST_PIXEL:])\n",
    "\n",
    "  label = -1 if image[DIGIT] == 1 else 1\n",
    "\n",
    "  new_train_df.append([label, intensity_, symmetry_])\n",
    "\n",
    "new_train_df = np.array(new_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_df = []\n",
    "images = get_numbers_1_and_5(np_test_df)\n",
    "\n",
    "for image in images:\n",
    "  symmetry_ = symmetry(image[FIRST_PIXEL:])\n",
    "  intensity_ = intensity(image[FIRST_PIXEL:])\n",
    "\n",
    "  label = -1 if image[DIGIT] == 1 else 1\n",
    "\n",
    "  new_test_df.append([label, intensity_, symmetry_,])\n",
    "\n",
    "new_test_df = np.array(new_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "  def __init__(self, max_iter):   \n",
    "    self.max_iter = max_iter         \n",
    "\n",
    "  def fit(self, _X, _y):              #Treino\n",
    "    dimension = len(_X[0])            #Dimensão do vetor de entrada\n",
    "    self.w = 2 * np.random.random(size=dimension) - 1  #Pesos aleatórios\n",
    "    self.best_error = dimension       #Erro inicial\n",
    "    best_w = self.w                   #Melhores pesos\n",
    "    \n",
    "    for i in range(self.max_iter):    #Iterações\n",
    "\n",
    "      for x_n, y_n in zip(_X, _y):    #Percorre os pontos\n",
    "        y_pred = np.sign(np.dot(x_n[1:], self.w[1:]) + (x_n[0] * self.w[0])) #Calcula a saída do perceptron\n",
    "\n",
    "        if y_pred != y_n:             #Se a saída for diferente do esperado\n",
    "          self.w = self.w + x_n * y_n #Atualiza os pesos\n",
    "          error = self.__error_in(zip(_X, _y)) #Calcula o erro\n",
    "          if self.best_error > error: #Se o erro for menor que o melhor erro\n",
    "            self.best_error = error   #Atualiza o melhor erro\n",
    "            best = self.w             #Atualiza os melhores pesos\n",
    "      \n",
    "      self.w = best_w                 #Atualiza os pesos\n",
    "\n",
    "  def predict(self, x_test, first_digit, second_digit):\n",
    "    y_pred = np.sign(np.dot(x_test, self.w))\n",
    "    return np.where(y_pred == -1.0, first_digit, second_digit)\n",
    "\n",
    "  def get_weights(self):\n",
    "    return self.w[1:]\n",
    "\n",
    "  def get_bias(self):\n",
    "    return self.w[0]\n",
    "\n",
    "  # Private\n",
    "\n",
    "  def __error_in(self, points):\n",
    "    error = 0\n",
    "\n",
    "    for x_n, y_n in points:\n",
    "      y_pred = np.sign(np.dot(x_n, self.w))\n",
    "      error += 1 if y_pred != y_n else 0\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import random\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, eta=0.1, tmax=1000, batch_size=50, lam =0):\n",
    "        self.eta = eta\n",
    "        self.tmax = tmax\n",
    "        self.batch_size = batch_size\n",
    "        self.lam = lam\n",
    "\n",
    "    # Infere o vetor w da funçao hipotese\n",
    "    #Executa a minimizao do erro de entropia cruzada pelo algoritmo gradiente de descida\n",
    "    def fit(self, _X, _y):\n",
    "        self.w = []\n",
    "        X = np.concatenate((np.ones((len(_X),1)), _X), axis=1)\n",
    "        y = np.array(_y)\n",
    "        \n",
    "        d = X.shape[1]\n",
    "        N = X.shape[0]\n",
    "        w = np.zeros(d, dtype=float)\n",
    "        self.w = []\n",
    "        \n",
    "        for i in range(self.tmax):\n",
    "            vsoma = np.zeros(d, dtype=float)\n",
    "\n",
    "            #Escolhendo o lote de entradas\n",
    "            if self.batch_size < N:\n",
    "                indices = random.sample(range(N),self.batch_size)\n",
    "                batchX = [X[index] for index in indices]\n",
    "                batchY = [y[index] for index in indices]\n",
    "            else:\n",
    "                batchX = X\n",
    "                batchY = y\n",
    "\n",
    "            #computando o gradiente no ponto atual\n",
    "            for xn, yn in zip(batchX, batchY):\n",
    "                vsoma += (yn * xn) / (1 + np.exp((yn * w).T @ xn))\n",
    "            \n",
    "            gt = vsoma/self.batch_size\n",
    "\n",
    "            if self.lam != 0:\n",
    "                gt += 2*self.lam * w\n",
    "            \n",
    "            w = w + (self.eta*gt)\n",
    "\n",
    "            #Condicao de parada: se ||deltaF|| < epsilon (0.0001)\n",
    "            if LA.norm(gt) < 0.0001 :\n",
    "                break\n",
    "            w = w + (self.eta*gt)\n",
    "\n",
    "        self.w = w\n",
    "    \n",
    "    # def _sigmoid(self, x):\n",
    "    #     return 1 / (1 + np.exp(-(self.w[0] + self.w[1:].T @ x)))\n",
    "    def _sigmoid(self, x):\n",
    "        return np.exp(-(self.w[0] + self.w[1:].T @ x)) / (1 + np.exp((self.w[0] + self.w[1:].T @ x)))\n",
    "\n",
    "    #funcao hipotese inferida pela regressa logistica  \n",
    "    def predict_prob(self, X):\n",
    "        pred = [self._sigmoid(x) for x in X]\n",
    "        return pred\n",
    "\n",
    "    #Predicao por classificação linear\n",
    "    def predict(self, X):\n",
    "        pred_classifier = [1 if self._sigmoid(x) >= 0.5 else -1 for x in X]\n",
    "        return pred_classifier\n",
    "\n",
    "    def getW(self):\n",
    "        return self.w\n",
    "\n",
    "    def getRegressionY(self, regressionX, shift=0):\n",
    "        return (-self.w[0]+shift - self.w[1]*regressionX) / self.w[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLA_predict(X_test,w):\n",
    "\n",
    "    preds = np.sign(np.dot(X_test,w))\n",
    "\n",
    "    return np.where(preds == 1.0, 1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERCEPTRON\n",
    "\n",
    "pla = Perceptron(1)\n",
    "pla.fit(add_1_column(new_train_df[:, 1:]), new_train_df[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37657736, 0.31077836])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pla.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29784607276618535"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pla.get_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pla.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49021/2858402931.py:38: RuntimeWarning: overflow encountered in exp\n",
      "  vsoma += (yn * xn) / (1 + np.exp((yn * w).T @ xn))\n"
     ]
    }
   ],
   "source": [
    "# LOGISTIC REGRESSION\n",
    "\n",
    "log_r = LogisticRegression()\n",
    "log_r.fit(add_1_column(new_train_df[:, 1:]), new_train_df[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.54      1.00      0.70      1528\n",
      "         1.0       0.00      0.00      0.00      1324\n",
      "\n",
      "    accuracy                           0.54      2852\n",
      "   macro avg       0.27      0.50      0.35      2852\n",
      "weighted avg       0.29      0.54      0.37      2852\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_49021/2858402931.py:57: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(-(self.w[0] + self.w[1:].T @ x)) / (1 + np.exp((self.w[0] + self.w[1:].T @ x)))\n",
      "/home/messias/miniconda3/envs/ufpb/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/messias/miniconda3/envs/ufpb/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/messias/miniconda3/envs/ufpb/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(new_test_df[:, 0], log_r.predict(add_1_column(new_test_df))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_r = LogisticRegression()\n",
    "log_r.fit(new_train_df[:, 1:], new_train_df[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        -1.0       0.82      0.88      0.85      1528\n",
      "         1.0       0.85      0.77      0.81      1324\n",
      "\n",
      "    accuracy                           0.83      2852\n",
      "   macro avg       0.83      0.83      0.83      2852\n",
      "weighted avg       0.83      0.83      0.83      2852\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(new_test_df[:, 0], log_r.predict(new_test_df[:, 1:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8302945301542777"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_r.score(new_test_df[:, 1:], new_test_df[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/messias/Documents/UFPB/7º Período/Aprendizagem de Máquina/project/perceptron_teste.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/messias/Documents/UFPB/7%C2%BA%20Per%C3%ADodo/Aprendizagem%20de%20M%C3%A1quina/project/perceptron_teste.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/messias/Documents/UFPB/7%C2%BA%20Per%C3%ADodo/Aprendizagem%20de%20M%C3%A1quina/project/perceptron_teste.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(log_r\u001b[39m.\u001b[39;49mpredict(new_test_df[:, \u001b[39m1\u001b[39;49m:]))\n",
      "\u001b[1;32m/home/messias/Documents/UFPB/7º Período/Aprendizagem de Máquina/project/perceptron_teste.ipynb Cell 18\u001b[0m in \u001b[0;36mLogisticRegression.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/messias/Documents/UFPB/7%C2%BA%20Per%C3%ADodo/Aprendizagem%20de%20M%C3%A1quina/project/perceptron_teste.ipynb#X24sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/messias/Documents/UFPB/7%C2%BA%20Per%C3%ADodo/Aprendizagem%20de%20M%C3%A1quina/project/perceptron_teste.ipynb#X24sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     pred_classifier \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sigmoid(x) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/messias/Documents/UFPB/7%C2%BA%20Per%C3%ADodo/Aprendizagem%20de%20M%C3%A1quina/project/perceptron_teste.ipynb#X24sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pred_classifier\n",
      "\u001b[1;32m/home/messias/Documents/UFPB/7º Período/Aprendizagem de Máquina/project/perceptron_teste.ipynb Cell 18\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/messias/Documents/UFPB/7%C2%BA%20Per%C3%ADodo/Aprendizagem%20de%20M%C3%A1quina/project/perceptron_teste.ipynb#X24sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/messias/Documents/UFPB/7%C2%BA%20Per%C3%ADodo/Aprendizagem%20de%20M%C3%A1quina/project/perceptron_teste.ipynb#X24sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     pred_classifier \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sigmoid(x) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m X]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/messias/Documents/UFPB/7%C2%BA%20Per%C3%ADodo/Aprendizagem%20de%20M%C3%A1quina/project/perceptron_teste.ipynb#X24sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m pred_classifier\n",
      "\u001b[1;32m/home/messias/Documents/UFPB/7º Período/Aprendizagem de Máquina/project/perceptron_teste.ipynb Cell 18\u001b[0m in \u001b[0;36mLogisticRegression._sigmoid\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/messias/Documents/UFPB/7%C2%BA%20Per%C3%ADodo/Aprendizagem%20de%20M%C3%A1quina/project/perceptron_teste.ipynb#X24sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sigmoid\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/messias/Documents/UFPB/7%C2%BA%20Per%C3%ADodo/Aprendizagem%20de%20M%C3%A1quina/project/perceptron_teste.ipynb#X24sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw[\u001b[39m1\u001b[39;49m:]\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39;49m x)) \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mexp((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw[\u001b[39m0\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mw[\u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m x)))\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 3)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(log_r.predict(new_test_df[:, 1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ufpb')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c245dbadf8928f9440a814283e41d21fcdb6faae853c6ba3d972269b28d83b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
